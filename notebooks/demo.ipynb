{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Financial Document Reader - Demo\n",
        "\n",
        "This notebook demonstrates the Named Entity Recognition (NER) extraction capabilities of the Financial Document Reader.\n",
        "\n",
        "## Features\n",
        "- Extract financial entities from PDF, DOCX, and TXT files\n",
        "- Rule-based extraction with regex patterns\n",
        "- Automatic normalization (amounts, dates, spreads)\n",
        "- Confidence scoring for each extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pydantic'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Add backend to path\u001b[39;00m\n\u001b[32m      6\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(Path.cwd().parent / \u001b[33m'\u001b[39m\u001b[33mbackend\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextractors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrule_based\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RuleBasedExtractor\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextractors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocumentProcessor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnormalizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFOCO-PROJECT/backend/app/extractors/rule_based.py:15\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Any, Pattern\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnormalizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     normalize_amount,\n\u001b[32m      8\u001b[39m     normalize_date,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     clean_text\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschemas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Entity\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRuleBasedExtractor\u001b[39;00m:\n\u001b[32m     19\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Rule-based entity extraction using regex patterns\"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFOCO-PROJECT/backend/app/models/schemas.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mPydantic models for API request/response validation\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Any, Dict\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pydantic'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Add backend to path\n",
        "sys.path.insert(0, str(Path.cwd().parent / 'backend'))\n",
        "\n",
        "from app.extractors.rule_based import RuleBasedExtractor\n",
        "from app.extractors.document_processor import DocumentProcessor\n",
        "from app.utils.normalizers import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Extract from Sample Text\n",
        "\n",
        "Let's extract entities from the provided trade confirmation text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_text = \"\"\"\n",
        "11:49:05 I'll revert regarding BANK ABC to try to do another 200 mio at 2Y\n",
        "FR001400QV82    AVMAFC FLOAT    06/30/28\n",
        "offer 2Y EVG estr+45bps\n",
        "estr average Estr average / Quarterly interest payment\n",
        "\"\"\"\n",
        "\n",
        "# Initialize extractor\n",
        "extractor = RuleBasedExtractor()\n",
        "\n",
        "# Extract entities\n",
        "entities = extractor.extract(sample_text, source=\"sample_text\")\n",
        "\n",
        "print(f\"Found {len(entities)} entities:\\n\")\n",
        "\n",
        "for entity in entities:\n",
        "    print(f\"[{entity.entity}]\")\n",
        "    print(f\"   Raw: {entity.raw_value}\")\n",
        "    print(f\"   Normalized: {entity.normalized}\")\n",
        "    print(f\"   Confidence: {entity.confidence:.2%}\")\n",
        "    print(f\"   Position: {entity.char_start}-{entity.char_end}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Using Pre-trained NER Model (spaCy)\n",
        "\n",
        "This demonstrates using a general-purpose NER model for entity extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install spaCy if needed: pip install spacy\n",
        "# Download model: python -m spacy download en_core_web_sm\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    \n",
        "    # Load pre-trained NER model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    \n",
        "    # Process sample text\n",
        "    doc = nlp(sample_text)\n",
        "    \n",
        "    print(\"Entities detected by spaCy NER model:\\n\")\n",
        "    \n",
        "    for ent in doc.ents:\n",
        "        print(f\"Text: {ent.text}\")\n",
        "        print(f\"  Label: {ent.label_}\")\n",
        "        print(f\"  Description: {spacy.explain(ent.label_)}\")\n",
        "        print(f\"  Position: {ent.start_char}-{ent.end_char}\")\n",
        "        print()\n",
        "    \n",
        "    print(f\"\\nTotal entities found: {len(doc.ents)}\")\n",
        "    \n",
        "    # Note: Pre-trained models recognize general entities (PERSON, ORG, MONEY, DATE)\n",
        "    # For financial-specific entities (ISIN, Tenor, Spread), fine-tuning is needed\n",
        "    # See NER_FINETUNING_METHODOLOGY.md for details\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"spaCy not installed. Install with: pip install spacy\")\n",
        "    print(\"Then download model: python -m spacy download en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Comparison - Rule-Based vs NER Model\n",
        "\n",
        "Let's compare both approaches:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"COMPARISON: Rule-Based vs NER Model\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nRule-Based Approach:\")\n",
        "print(\"  Pros:\")\n",
        "print(\"    - Extracts domain-specific entities (ISIN, Tenor, Spread)\")\n",
        "print(\"    - Very fast (< 50ms)\")\n",
        "print(\"    - High precision for well-defined patterns\")\n",
        "print(\"    - No training data needed\")\n",
        "print(\"  Cons:\")\n",
        "print(\"    - Brittle to format variations\")\n",
        "print(\"    - Requires manual pattern updates\")\n",
        "print(\"    - Limited context understanding\")\n",
        "\n",
        "print(\"\\nNER Model Approach:\")\n",
        "print(\"  Pros:\")\n",
        "print(\"    - Handles format variations better\")\n",
        "print(\"    - Understands linguistic context\")\n",
        "print(\"    - Learns from data\")\n",
        "print(\"    - Generalizes to new patterns\")\n",
        "print(\"  Cons:\")\n",
        "print(\"    - Requires training data (500-1000 examples)\")\n",
        "print(\"    - Slower inference (~100-500ms)\")\n",
        "print(\"    - May need fine-tuning for financial entities\")\n",
        "print(\"    - Higher computational requirements\")\n",
        "\n",
        "print(\"\\nRecommended Hybrid Approach:\")\n",
        "print(\"  1. Use rule-based for high-confidence patterns (ISIN, amounts)\")\n",
        "print(\"  2. Use NER model for ambiguous cases (counterparties, context)\")\n",
        "print(\"  3. Combine with confidence scoring\")\n",
        "print(\"  4. Post-process and validate results\")\n",
        "\n",
        "print(\"\\nFor fine-tuning NER models, see: NER_FINETUNING_METHODOLOGY.md\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Test Normalizers\n",
        "\n",
        "Demonstrate the normalization functions for different data types.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Amount Normalization:\")\n",
        "print(f\"  '200 mio' -> {normalize_amount('200 mio')}\")\n",
        "print(f\"  'EUR 1 million' -> {normalize_amount('EUR 1 million')}\")\n",
        "print(f\"  '500k' -> {normalize_amount('500k')}\")\n",
        "\n",
        "print(\"\\nDate Normalization:\")\n",
        "print(f\"  '31 January 2025' -> {normalize_date('31 January 2025')}\")\n",
        "print(f\"  '06/30/28' -> {normalize_date('06/30/28')}\")\n",
        "\n",
        "print(\"\\nSpread Normalization:\")\n",
        "print(f\"  'estr+45bps' -> {normalize_spread('estr+45bps')}\")\n",
        "print(f\"  'libor+100' -> {normalize_spread('libor+100')}\")\n",
        "\n",
        "print(\"\\nPercentage Normalization:\")\n",
        "print(f\"  '75%' -> {normalize_percentage('75%')}\")\n",
        "print(f\"  '0.5%' -> {normalize_percentage('0.5%')}\")\n",
        "\n",
        "print(\"\\nTenor Normalization:\")\n",
        "print(f\"  '2Y' -> {normalize_tenor('2Y')}\")\n",
        "print(f\"  '6M' -> {normalize_tenor('6M')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This demo showed:\n",
        "1. Rule-based entity extraction with regex\n",
        "2. Automatic normalization of amounts, dates, and spreads\n",
        "3. Confidence scoring for each extraction\n",
        "4. Multi-format support (TXT, PDF, DOCX)\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Upload your own documents through the web UI at http://localhost:3000\n",
        "- Use the REST API at http://localhost:8000/docs\n",
        "- Extend the extraction patterns in `backend/app/extractors/rule_based.py`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
