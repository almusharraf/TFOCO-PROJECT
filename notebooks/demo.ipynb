{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TFOCO Financial Document Reader - Demo\n",
        "\n",
        "This notebook demonstrates the Named Entity Recognition (NER) extraction capabilities of the TFOCO Financial Document Reader.\n",
        "\n",
        "## Features\n",
        "- Extract financial entities from PDF, DOCX, and TXT files\n",
        "- Rule-based extraction with regex patterns\n",
        "- Automatic normalization (amounts, dates, spreads)\n",
        "- Confidence scoring for each extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Add backend to path\n",
        "sys.path.insert(0, str(Path.cwd().parent / 'backend'))\n",
        "\n",
        "from app.extractors.rule_based import RuleBasedExtractor\n",
        "from app.extractors.document_processor import DocumentProcessor\n",
        "from app.utils.normalizers import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Extract from Sample Text\n",
        "\n",
        "Let's extract entities from the provided trade confirmation text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_text = \"\"\"\n",
        "11:49:05 I'll revert regarding BANK ABC to try to do another 200 mio at 2Y\n",
        "FR001400QV82    AVMAFC FLOAT    06/30/28\n",
        "offer 2Y EVG estr+45bps\n",
        "estr average Estr average / Quarterly interest payment\n",
        "\"\"\"\n",
        "\n",
        "# Initialize extractor\n",
        "extractor = RuleBasedExtractor()\n",
        "\n",
        "# Extract entities\n",
        "entities = extractor.extract(sample_text, source=\"sample_text\")\n",
        "\n",
        "print(f\"Found {len(entities)} entities:\\n\")\n",
        "\n",
        "for entity in entities:\n",
        "    print(f\"[{entity.entity}]\")\n",
        "    print(f\"   Raw: {entity.raw_value}\")\n",
        "    print(f\"   Normalized: {entity.normalized}\")\n",
        "    print(f\"   Confidence: {entity.confidence:.2%}\")\n",
        "    print(f\"   Position: {entity.char_start}-{entity.char_end}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Using Pre-trained NER Model (spaCy)\n",
        "\n",
        "This demonstrates using a general-purpose NER model for entity extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install spaCy if needed: pip install spacy\n",
        "# Download model: python -m spacy download en_core_web_sm\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    \n",
        "    # Load pre-trained NER model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    \n",
        "    # Process sample text\n",
        "    doc = nlp(sample_text)\n",
        "    \n",
        "    print(\"Entities detected by spaCy NER model:\\n\")\n",
        "    \n",
        "    for ent in doc.ents:\n",
        "        print(f\"Text: {ent.text}\")\n",
        "        print(f\"  Label: {ent.label_}\")\n",
        "        print(f\"  Description: {spacy.explain(ent.label_)}\")\n",
        "        print(f\"  Position: {ent.start_char}-{ent.end_char}\")\n",
        "        print()\n",
        "    \n",
        "    print(f\"\\nTotal entities found: {len(doc.ents)}\")\n",
        "    \n",
        "    # Note: Pre-trained models recognize general entities (PERSON, ORG, MONEY, DATE)\n",
        "    # For financial-specific entities (ISIN, Tenor, Spread), fine-tuning is needed\n",
        "    # See NER_FINETUNING_METHODOLOGY.md for details\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"spaCy not installed. Install with: pip install spacy\")\n",
        "    print(\"Then download model: python -m spacy download en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Comparison - Rule-Based vs NER Model\n",
        "\n",
        "Let's compare both approaches:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"COMPARISON: Rule-Based vs NER Model\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nRule-Based Approach:\")\n",
        "print(\"  Pros:\")\n",
        "print(\"    - Extracts domain-specific entities (ISIN, Tenor, Spread)\")\n",
        "print(\"    - Very fast (< 50ms)\")\n",
        "print(\"    - High precision for well-defined patterns\")\n",
        "print(\"    - No training data needed\")\n",
        "print(\"  Cons:\")\n",
        "print(\"    - Brittle to format variations\")\n",
        "print(\"    - Requires manual pattern updates\")\n",
        "print(\"    - Limited context understanding\")\n",
        "\n",
        "print(\"\\nNER Model Approach:\")\n",
        "print(\"  Pros:\")\n",
        "print(\"    - Handles format variations better\")\n",
        "print(\"    - Understands linguistic context\")\n",
        "print(\"    - Learns from data\")\n",
        "print(\"    - Generalizes to new patterns\")\n",
        "print(\"  Cons:\")\n",
        "print(\"    - Requires training data (500-1000 examples)\")\n",
        "print(\"    - Slower inference (~100-500ms)\")\n",
        "print(\"    - May need fine-tuning for financial entities\")\n",
        "print(\"    - Higher computational requirements\")\n",
        "\n",
        "print(\"\\nRecommended Hybrid Approach:\")\n",
        "print(\"  1. Use rule-based for high-confidence patterns (ISIN, amounts)\")\n",
        "print(\"  2. Use NER model for ambiguous cases (counterparties, context)\")\n",
        "print(\"  3. Combine with confidence scoring\")\n",
        "print(\"  4. Post-process and validate results\")\n",
        "\n",
        "print(\"\\nFor fine-tuning NER models, see: NER_FINETUNING_METHODOLOGY.md\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Test Normalizers\n",
        "\n",
        "Demonstrate the normalization functions for different data types.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Amount Normalization:\")\n",
        "print(f\"  '200 mio' -> {normalize_amount('200 mio')}\")\n",
        "print(f\"  'EUR 1 million' -> {normalize_amount('EUR 1 million')}\")\n",
        "print(f\"  '500k' -> {normalize_amount('500k')}\")\n",
        "\n",
        "print(\"\\nDate Normalization:\")\n",
        "print(f\"  '31 January 2025' -> {normalize_date('31 January 2025')}\")\n",
        "print(f\"  '06/30/28' -> {normalize_date('06/30/28')}\")\n",
        "\n",
        "print(\"\\nSpread Normalization:\")\n",
        "print(f\"  'estr+45bps' -> {normalize_spread('estr+45bps')}\")\n",
        "print(f\"  'libor+100' -> {normalize_spread('libor+100')}\")\n",
        "\n",
        "print(\"\\nPercentage Normalization:\")\n",
        "print(f\"  '75%' -> {normalize_percentage('75%')}\")\n",
        "print(f\"  '0.5%' -> {normalize_percentage('0.5%')}\")\n",
        "\n",
        "print(\"\\nTenor Normalization:\")\n",
        "print(f\"  '2Y' -> {normalize_tenor('2Y')}\")\n",
        "print(f\"  '6M' -> {normalize_tenor('6M')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This demo showed:\n",
        "1. Rule-based entity extraction with regex\n",
        "2. Automatic normalization of amounts, dates, and spreads\n",
        "3. Confidence scoring for each extraction\n",
        "4. Multi-format support (TXT, PDF, DOCX)\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Upload your own documents through the web UI at http://localhost:3000\n",
        "- Use the REST API at http://localhost:8000/docs\n",
        "- Extend the extraction patterns in `backend/app/extractors/rule_based.py`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
